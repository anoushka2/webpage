
@article{DBLP:journals/corr/abs-2104-10263,
  author    = {Alexander Spangher and
               Jonathan May},
  title     = {{\textbackslash}textit\{StateCensusLaws.org\}: {A} Web Application
               for Consuming and Annotating Legal Discourse Learning},
  journal   = {CoRR},
  volume    = {abs/2104.10263},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.10263},
  eprinttype = {arXiv},
  eprint    = {2104.10263},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-10263.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2108-11063,
  author    = {Hyundong Cho and
               Basel Shbita and
               Kartik Shenoy and
               Shuai Liu and
               Nikhil Patel and
               Hitesh Pindikanti and
               Jennifer Lee and
               Jonathan May},
  title     = {Viola: {A} Topic Agnostic Generate-and-Rank Dialogue System},
  journal   = {CoRR},
  volume    = {abs/2108.11063},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.11063},
  eprinttype = {arXiv},
  eprint    = {2108.11063},
  timestamp = {Fri, 27 Aug 2021 15:02:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-11063.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mirzaalian2021explaining,
      title={Explaining Face Presentation Attack Detection Using Natural Language}, 
      author={Hengameh Mirzaalian and Mohamed E. Hussein and Leonidas Spinoulas and Jonathan May and Wael Abd-Almageed},
      year={2021},
      eprint={2111.04862},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{10.1145/3465413.3488575,
author = {Weideman, Nicolaas and Felkner, Virginia K. and Wu, Wei-Cheng and May, Jonathan and Hauser, Christophe and Garcia, Luis},
title = {PERFUME: Programmatic Extraction and Refinement for Usability of Mathematical Expression},
year = {2021},
isbn = {9781450385527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465413.3488575},
doi = {10.1145/3465413.3488575},
abstract = {Algorithmic identification is the crux for several binary analysis applications, including malware analysis, vulnerability discovery, and embedded firmware reverse engineering. However, data-driven and signature-based approaches often break down when encountering outlier realizations of a particular algorithm. Moreover, reverse engineering of domain-specific binaries often requires collaborative analysis between reverse engineers and domain experts. Communicating the behavior of an unidentified binary program to non-reverse engineers necessitates the recovery of algorithmic semantics in a human-digestible form. This paper presents PERFUME, a framework that extracts symbolic math expressions from low-level binary representations of an algorithm. PERFUME works by translating a symbolic output representation of a binary function to a high-level mathematical expression. In particular, we detail how source and target representations are generated for training a machine translation model. We integrate PERFUME as a plug-in for Ghidra--an open-source reverse engineering framework. We present our preliminary findings for domain-specific use cases and formalize open challenges in mathematical expression extraction from algorithmic implementations.},
booktitle = {Proceedings of the 2021 Research on Offensive and Defensive Techniques in the Context of Man At The End (MATE) Attacks},
pages = {59–69},
numpages = {11},
keywords = {reverse engineering, binary analysis},
location = {Virtual Event, Republic of Korea},
series = {Checkmate '21}
}



@article{DBLP:journals/corr/abs-2106-01540,
  author    = {Xuezhe Ma and
               Xiang Kong and
               Sinong Wang and
               Chunting Zhou and
               Jonathan May and
               Hao Ma and
               Luke Zettlemoyer},
  title     = {Luna: Linear Unified Nested Attention},
  journal   = {CoRR},
  volume    = {abs/2106.01540},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.01540},
  eprinttype = {arXiv},
  eprint    = {2106.01540},
  timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-01540.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{spangher-etal-2021-multitask,
    title = "Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification",
    author = "Spangher, Alexander  and
      May, Jonathan  and
      Shiang, Sz-Rung  and
      Deng, Lingjia",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.40",
    pages = "498--517",
    abstract = "As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In this work, we show that a multitask learning approach can combine discourse datasets from similar and diverse domains to improve discourse classification. We show an improvement of 4.9{\%} Micro F1-score over current state-of-the-art benchmarks on the \textit{NewsDiscourse} dataset, one of the largest discourse datasets recently published, due in part to label correlations across tasks, which improve performance for underrepresented classes. We also offer an extensive review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in our setting.",
}

@inproceedings{zhang-etal-2021-salience,
    title = "Salience-Aware Event Chain Modeling for Narrative Understanding",
    author = "Zhang, Xiyang  and
      Chen, Muhao  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.107",
    pages = "1418--1428",
    abstract = "Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction remains a challenging problem. We posit that this is due to the nature of the texts from which chains are discovered. Natural language text interleaves a narrative of concrete, salient events with background information, contextualization, opinion, and other elements that are important for a variety of necessary discourse and pragmatics acts but are not part of the principal chain of events being communicated. We introduce methods for extracting this principal chain from natural language text, by filtering away non-salient events and supportive sentences. We demonstrate the effectiveness of our methods at isolating critical event chains by comparing their effect on downstream tasks. We show that by pre-training large language models on our extracted chains, we obtain improvements in two tasks that benefit from a clear understanding of event chains: narrative prediction and event-based temporal question answering. The demonstrated improvements and ablative studies confirm that our extraction method isolates critical event chains.",
}

@inproceedings{gheini-etal-2021-cross,
    title = "Cross-Attention is All You Need: {A}dapting Pretrained {T}ransformers for Machine Translation",
    author = "Gheini, Mozhdeh  and
      Ren, Xiang  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.132",
    pages = "1754--1765",
    abstract = "We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.",
}


@inproceedings{yin-etal-2021-summary,
    title = "Summary-Oriented Question Generation for Informational Queries",
    author = "Yin, Xusen  and
      Zhou, Li  and
      Small, Kevin  and
      May, Jonathan",
    booktitle = "Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.dialdoc-1.11",
    pages = "81--97",
    abstract = "Users frequently ask simple factoid questions for question answering (QA) systems, attenuating the impact of myriad recent works that support more complex questions. Prompting users with automatically generated suggested questions (SQs) can improve user understanding of QA system capabilities and thus facilitate more effective use. We aim to produce self-explanatory questions that focus on main document topics and are answerable with variable length passages as appropriate. We satisfy these requirements by using a BERT-based Pointer-Generator Network trained on the Natural Questions (NQ) dataset. Our model shows SOTA performance of SQ generation on the NQ dataset (20.1 BLEU-4). We further apply our model on out-of-domain news articles, evaluating with a QA system due to the lack of gold questions and demonstrate that our model produces better SQs for news articles {--} with further confirmation via a human evaluation.",
}



@misc{gheini2021strengths,
      title={On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation}, 
      author={Mozhdeh Gheini and Xiang Ren and Jonathan May},
      year={2021},
      eprint={2104.08771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{DBLP:journals/corr/abs-1909-06516,
  author    = {Mozhdeh Gheini and
               Jonathan May},
  title     = {A Universal Parent Model for Low-Resource Neural Machine Translation
               Transfer},
  journal   = {CoRR},
  volume    = {abs/1909.06516},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.06516},
  archivePrefix = {arXiv},
  eprint    = {1909.06516},
  timestamp = {Mon, 23 Sep 2019 18:07:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-06516.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gowda-etal-2021-many,
    title = "Many-to-{E}nglish Machine Translation Tools, Data, and Pretrained Models",
    author = "Gowda, Thamme  and
      Zhang, Zhao  and
      Mattmann, Chris  and
      May, Jonathan",
    booktitle = "Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.37",
    pages = "306--316",
    abstract = "While there are more than 7000 languages in the world, most translation research efforts have targeted a few high resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages.",
}


@inproceedings{hambardzumyan-etal-2021-warp,
    title = "{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming",
    author = "Hambardzumyan, Karen  and
      Khachatrian, Hrant  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.381",
    pages = "4921--4933",
    abstract = "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
}

@inproceedings{aldarrab-may-2021-sequence,
    title = "Can Sequence-to-Sequence Models Crack Substitution Ciphers?",
    author = "Aldarrab, Nada  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.561",
    pages = "7226--7235",
    abstract = "Decipherment of historical ciphers is a challenging problem. The language of the target plaintext might be unknown, and ciphertext can have a lot of noise. State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known. We propose an end-to-end multilingual model for solving simple substitution ciphers. We test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise.",
}

@inproceedings{mhamdi-etal-2021-x,
    title = "{X}-{METRA}-{ADA}: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering",
    author = "M{'}hamdi, Meryem  and
      Kim, Doo Soon  and
      Dernoncourt, Franck  and
      Bui, Trung  and
      Ren, Xiang  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.283",
    pages = "3617--3632",
    abstract = "Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.",
}


@inproceedings{chawla-etal-2021-casino,
    title = "{C}a{S}i{N}o: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems",
    author = "Chawla, Kushal  and
      Ramirez, Jaysa  and
      Clever, Rene  and
      Lucas, Gale  and
      May, Jonathan  and
      Gratch, Jonathan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.254",
    pages = "3167--3185",
    abstract = "Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo",
}

@inproceedings{gowda-etal-2021-macro,
    title = "Macro-Average: Rare Types Are Important Too",
    author = "Gowda, Thamme  and
      You, Weiqiu  and
      Lignos, Constantine  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.90",
    pages = "1138--1157",
    abstract = "While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods{'} outputs.",
}



@proceedings{semeval-2020-semantic,
    title = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.semeval-1.0",
}



@InBook{mythodologies-2019,
  author =    {Joseph A. Dane and Jonathan May},
  title =        {Begging The Question: Critical Reasoning in Chaucer Studies, Book History, and Humanistic Inquiry},
  chapter =      {III.3: Evidence and Artificial Intelligence},
  publisher =    {Marymount Institute Press},
  year =         2019,
  number =    {II},
  series =    {Mythodologies},
  pages =     {295--208}}

@proceedings{nlp-covid19-2020-nlp,
    title = "Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020",
    editor = "Verspoor, Karin  and
      Cohen, Kevin Bretonnel  and
      Dredze, Mark  and
      Ferrara, Emilio  and
      May, Jonathan  and
      Munro, Robert  and
      Paris, Cecile  and
      Wallace, Byron",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlpcovid19-acl.0",
}

@inproceedings{spangher-etal-2020-enabling,
    title = "Enabling Low-Resource Transfer Learning across {COVID-19} Corpora by Combining Event-Extraction and Co-Training",
    author = "Spangher, Alexander  and
      Peng, Nanyun  and
      May, Jonathan  and
      Ferrara, Emilio",
    booktitle = "Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlpcovid19-acl.4",
}

@inproceedings{yin-etal-2020-learning,
    title = "Learning to Generalize for Sequential Decision Making",
    author = "Yin, Xusen  and
      Weischedel, Ralph  and
      May, Jonathan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.273",
    pages = "3046--3063",
    abstract = "We consider problems of making sequences of decisions to accomplish tasks, interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied to novel task domains. However, the large amount of computation necessary to adequately train and explore the search space of sequential decision making, under a reinforcement learning paradigm, precludes the inclusion of large contextualized language models, which might otherwise enable the desired generalization ability. We introduce a teacher-student imitation learning methodology and a means of converting a reinforcement learning model into a natural language understanding model. Together, these methodologies enable the introduction of contextualized language models into the sequential decision making problem space. We show that models can learn faster and generalize more, leveraging both the imitation learning and the reformulation. Our models exceed teacher performance on various held-out decision problems, by up to 7{\%} on in-domain problems and 24{\%} on out-of-domain problems.",
}

@inproceedings{gowda-may-2020-finding,
    title = "Finding the Optimal Vocabulary Size for Neural Machine Translation",
    author = "Gowda, Thamme  and
      May, Jonathan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.352",
    pages = "3955--3964",
    abstract = "We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various vocabulary sizes on NMT performance on multiple languages with many data sizes, and reveal an explanation for why certain vocabulary sizes are better than others.",
}

@inproceedings{li-etal-2020-connecting,
    title = "Connecting the Dots: Event Graph Schema Induction with Path Language Modeling",
    author = "Li, Manling  and
      Zeng, Qi  and
      Lin, Ying  and
      Cho, Kyunghyun  and
      Ji, Heng  and
      May, Jonathan  and
      Chambers, Nathanael  and
      Voss, Clare",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.50",
    pages = "684--695",
    abstract = "Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.",
}

@inproceedings{bisk-etal-2020-experience,
    title = "Experience Grounds Language",
    author = "Bisk, Yonatan  and
      Holtzman, Ari  and
      Thomason, Jesse  and
      Andreas, Jacob  and
      Bengio, Yoshua  and
      Chai, Joyce  and
      Lapata, Mirella  and
      Lazaridou, Angeliki  and
      May, Jonathan  and
      Nisnevich, Aleksandr  and
      Pinto, Nicolas  and
      Turian, Joseph",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.703",
    pages = "8718--8735",
    abstract = "Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.",
}


@inproceedings{cho-may-2020-grounding,
    title = "Grounding Conversations with Improvised Dialogues",
    author = "Cho, Hyundong  and
      May, Jonathan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.218",
    doi = "10.18653/v1/2020.acl-main.218",
    pages = "2398--2413",
    abstract = "Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.",
}


@InProceedings{spangher-2020-quote,
  author =       {Alexander Spangher, Nanyun Peng, Jonathan May, and Emilio Ferrara},
  title =        {“Don’t quote me on that”: Finding Mixtures of Sources in
News Articles},
  booktitle = {Proc. Computation+Journalism Symposium},
  year =      2020,
  month =     {March},
  address =   {Online}}

@inproceedings{lu-etal-2020-cross,
    title = "Cross-lingual Structure Transfer for Zero-resource Event Extraction",
    author = "Lu, Di  and
      Subburathinam, Ananya  and
      Ji, Heng  and
      May, Jonathan  and
      Chang, Shih-Fu  and
      Sil, Avi  and
      Voss, Clare",
    booktitle = "Proceedings of The 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.243",
    pages = "1976--1981",
    abstract = "Most of the current cross-lingual transfer learning methods for Information Extraction (IE) have been only applied to name tagging. To tackle more complex tasks such as event extraction we need to transfer graph structures (event trigger linked to multiple arguments with various roles) across languages. We develop a novel share-and-transfer framework to reach this goal with three steps: (1) Convert each sentence in any language to language-universal graph structures; in this paper we explore two approaches based on universal dependency parses and complete graphs, respectively. (2) Represent each node in the graph structure with a cross-lingual word embedding so that all sentences in multiple languages can be represented with one shared semantic space. (3) Using this common semantic space, train event extractors from English training data and apply them to languages that do not have any event annotations. Experimental results on three languages (Spanish, Russian and Ukrainian) without any annotations show this framework achieves comparable performance to a state-of-the-art supervised model trained from more than 1,500 manually annotated event mentions.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{pan-etal-2019-cross,
    title = "Cross-lingual Joint Entity and Word Embedding to Improve Entity Linking and Parallel Sentence Mining",
    author = "Pan, Xiaoman  and
      Gowda, Thamme  and
      Ji, Heng  and
      May, Jonathan  and
      Miller, Scott",
    booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-6107",
    doi = "10.18653/v1/D19-6107",
    pages = "56--66",
    abstract = "Entities, which refer to distinct objects in the real world, can be viewed as language universals and used as effective signals to generate less ambiguous semantic representations and align multiple languages. We propose a novel method, CLEW, to generate cross-lingual data that is a mix of entities and contextual words based on Wikipedia. We replace each anchor link in the source language with its corresponding entity title in the target language if it exists, or in the source language otherwise. A cross-lingual joint entity and word embedding learned from this kind of data not only can disambiguate linkable entities but can also effectively represent unlinkable entities. Because this multilingual common space directly relates the semantics of contextual words in the source language to that of entities in the target language, we leverage it for unsupervised cross-lingual entity linking. Experimental results show that CLEW significantly advances the state-of-the-art: up to 3.1{\%} absolute F-score gain for unsupervised cross-lingual entity linking. Moreover, it provides reliable alignment on both the word/entity level and the sentence level, and thus we use it to mine parallel sentences for all (302, 2) language pairs in Wikipedia.",
}

@inproceedings{mhamdi-etal-2019-contextualized,
    title = "Contextualized Cross-Lingual Event Trigger Extraction with Minimal Resources",
    author = "M{'}hamdi, Meryem  and
      Freedman, Marjorie  and
      May, Jonathan",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K19-1061",
    doi = "10.18653/v1/K19-1061",
    pages = "656--665",
    abstract = "Event trigger extraction is an information extraction task of practical utility, yet it is challenging due to the difficulty of disambiguating word sense meaning. Previous approaches rely extensively on hand-crafted language-specific features and are applied mainly to English for which annotated datasets and Natural Language Processing (NLP) tools are available. However, the availability of such resources varies from one language to another. Recently, contextualized Bidirectional Encoder Representations from Transformers (BERT) models have established state-of-the-art performance for a variety of NLP tasks. However, there has not been much effort in exploring language transfer using BERT for event extraction. In this work, we treat event trigger extraction as a sequence tagging problem and propose a cross-lingual framework for training it without any hand-crafted features. We experiment with different flavors of transfer learning from high-resourced to low-resourced languages and compare the performance of different multilingual embeddings for event trigger extraction. Our results show that training in a multilingual setting outperforms language-specific models for both English and Chinese. Our work is the first to experiment with two event architecture variants in a cross-lingual setting, to show the effectiveness of contextualized embeddings obtained using BERT, and to explore and analyze its performance on Arabic.",
}

@inproceedings{mullenbach-etal-2019-nuclear,
    title = "Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects",
    author = "Mullenbach, James  and
      Gordon, Jonathan  and
      Peng, Nanyun  and
      May, Jonathan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1625",
    doi = "10.18653/v1/D19-1625",
    pages = "6051--6057",
    abstract = "How do adjectives project from a noun to its parts? If a motorcycle is red, are its wheels red? Is a nuclear submarine{'}s captain nuclear? These questions are easy for humans to judge using our commonsense understanding of the world, but are difficult for computers. To attack this challenge, we crowdsource a set of human judgments that answer the English-language question {``}Given a whole described by an adjective, does the adjective also describe a given part?{''} We build strong baselines for this task with a classification approach. Our findings indicate that, despite the recent successes of large language models on tasks aimed to assess commonsense knowledge, these models do not greatly outperform simple word-level models based on pre-trained word embeddings. This provides evidence that the amount of commonsense knowledge encoded in these language models does not extend far beyond that already baked into the word embeddings. Our dataset will serve as a useful testbed for future research in commonsense reasoning, especially as it relates to adjectives and objects",
}

@inproceedings{huang-etal-2019-matters,
    title = "What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis",
    author = "Huang, Xiaolei  and
      May, Jonathan  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1672",
    doi = "10.18653/v1/D19-1672",
    pages = "6394--6400",
    abstract = "Building named entity recognition (NER) models for languages that do not have much training data is a challenging task. While recent work has shown promising results on cross-lingual transfer from high-resource languages, it is unclear what knowledge is transferred. In this paper, we first propose a simple and efficient neural architecture for cross-lingual NER. Experiments show that our model achieves competitive performance with the state-of-the-art. We further explore how transfer learning works for cross-lingual NER on two transferable factors: sequential order and multilingual embedding. Our results shed light on future research for improving cross-lingual NER.",
}

@inproceedings{subburathinam-etal-2019-cross,
    title = "Cross-lingual Structure Transfer for Relation and Event Extraction",
    author = "Subburathinam, Ananya  and
      Lu, Di  and
      Ji, Heng  and
      May, Jonathan  and
      Chang, Shih-Fu  and
      Sil, Avirup  and
      Voss, Clare",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1030",
    doi = "10.18653/v1/D19-1030",
    pages = "313--325",
    abstract = "The identification of complex semantic structures such as events and entity relations, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of cross-lingual structure transfer techniques for these tasks. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among English, Chinese, and Arabic demonstrate that our approach achieves performance comparable to state-of-the-art supervised models trained on up to 3,000 manually annotated mentions: up to 62.6{\%} F-score for Relation Extraction, and 63.1{\%} F-score for Event Argument Role Labeling. The event argument role labeling model transferred from English to Chinese achieves similar performance as the model trained from Chinese. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.",
}

@inproceedings{Yin2019ComprehensibleCT,
  title={Comprehensible Context-driven Text Game Playing},
  author={Xusen Yin and Jonathan May},
  booktitle={Proc. 2019 IEEE Conference on Games (CoG)},
  year={2019},
  month=August,
  pages={1-8}
}

@inproceedings{boschee-etal-2019-saral,
    title = "{SARAL}: A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage",
    author = "Boschee, Elizabeth  and
      Barry, Joel  and
      Billa, Jayadev  and
      Freedman, Marjorie  and
      Gowda, Thamme  and
      Lignos, Constantine  and
      Palen-Michel, Chester  and
      Pust, Michael  and
      Khonglah, Banriskhem Kayang  and
      Madikeri, Srikanth  and
      May, Jonathan  and
      Miller, Scott",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-3004",
    doi = "10.18653/v1/P19-3004",
    pages = "19--24",
    abstract = "With the increasing democratization of electronic media, vast information resources are available in less-frequently-taught languages such as Swahili or Somali. That information, which may be crucially important and not available elsewhere, can be difficult for monolingual English speakers to effectively access. In this paper we present an end-to-end cross-lingual information retrieval (CLIR) and summarization system for low-resource languages that 1) enables English speakers to search foreign language repositories of text and audio using English queries, 2) summarizes the retrieved documents in English with respect to a particular information need, and 3) provides complete transcriptions and translations as needed. The SARAL system achieved the top end-to-end performance in the most recent IARPA MATERIAL CLIR+summarization evaluations. Our demonstration system provides end-to-end open query retrieval and summarization capability, and presents the original source text or audio, speech transcription, and machine translation, for two low resource languages.",
}

@inproceedings{pourdamghani-etal-2019-translating,
    title = "Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation",
    author = "Pourdamghani, Nima  and
      Aldarrab, Nada  and
      Ghazvininejad, Marjan  and
      Knight, Kevin  and
      May, Jonathan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1293",
    doi = "10.18653/v1/P19-1293",
    pages = "3057--3062",
    abstract = "Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then {`}translating{'} the resulting pseudo-translation, or {`}Translationese{'} into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario.",
}

@proceedings{semeval-2019-international,
    title = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    editor = "May, Jonathan  and
      Shutova, Ekaterina  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S19-2000",
}

@InProceedings{huang-ji-may:2019:N19-1,
  author    = {Huang, Lifu  and  Ji, Heng  and  May, Jonathan},
  title     = {Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource Name Tagging},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {June},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  pages     = {3823--3833},
  abstract  = {We focus on improving name tagging for low-resource languages using annotations from related languages. Previous studies either directly project annotations from a source language to a target language using cross-lingual representations or use a shared encoder in a multitask network to transfer knowledge. These approaches inevitably introduce noise to the target language annotation due to mismatched source-target sentence structures. To effectively transfer the resources, we develop a new neural architecture that leverages multi-level adversarial transfer: (1) word-level adversarial training, which projects source language words into the same semantic space as those of the target language without using any parallel corpora or bilingual gazetteers, and (2) sentence-level adversarial training, which yields language-agnostic sequential features. Our neural architecture outperforms previous approaches on CoNLL data sets. Moreover, on 10 low-resource languages, our approach achieves up to 16\% absolute F-score gain over all high-performing baselines on cross-lingual transfer without using any target-language resources.},
  url       = {http://www.aclweb.org/anthology/N19-1383}
}

@InProceedings{cardenas-EtAl:2019:N19-1,
  author    = {Cardenas, Ronald  and  Lin, Ying  and  Ji, Heng  and  May, Jonathan},
  title     = {A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {June},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  pages     = {2428--2439},
  abstract  = {Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a `ciphertext' and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagger's utility by incorporating it into a true `zero-resource' variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our tagger makes up much of the accuracy lost when gold POS tags are unavailable.},
  url       = {http://www.aclweb.org/anthology/N19-1252}
}

@InProceedings{hermjakob-EtAl:2018:Demos,
  author    = {Hermjakob, Ulf  and  May, Jonathan  and  Pust, Michael  and  Knight, Kevin},
  title     = {Translating a Language You Don't Know In the Chinese Room},
  booktitle = {Proceedings of ACL 2018, System Demonstrations},
  month     = {July},
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {62--67},
  abstract  = {In a corruption of John Searle's famous AI thought experiment, the Chinese Room (Searle, 1980), we twist its original intent by enabling humans to translate text, e.g. from Uyghur to English, even if they don't have any prior knowledge of the source language. Our enabling tool, which we call the Chinese Room, is equipped with the same resources made available to a machine translation engine. We find that our superior language model and world knowledge allows us to create perfectly fluent and nearly adequate translations, with human expertise required only for the target language. The Chinese Room tool can be used to rapidly create small corpora of parallel data when bilingual translators are not readily available, in particular for low-resource languages.},
  url       = {http://www.aclweb.org/anthology/P18-4011}
}


@InProceedings{hermjakob-may-knight:2018:Demos,
  author    = {Hermjakob, Ulf  and  May, Jonathan  and  Knight, Kevin},
  title     = {Out-of-the-box Universal Romanization Tool uroman},
  booktitle = {Proceedings of ACL 2018, System Demonstrations},
  month     = {July},
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {13--18},
  abstract  = {We present uroman, a tool for converting text in myriads of languages and scripts such as Chinese, Arabic and Cyrillic into a common Latin-script representation. The tool relies on Unicode data and other tables, and handles nearly all character sets, including some that are quite obscure such as Tibetan and Tifinagh. uroman converts digital numbers in various scripts to Western Arabic numerals. Romanization enables the application of string-similarity metrics to texts from different scripts without the need and complexity of an intermediate phonetic representation. The tool is freely and publicly available as a Perl script suitable for inclusion in data processing pipelines and as an interactive demo web page.},
  url       = {http://www.aclweb.org/anthology/P18-4003}
}


@InProceedings{peng-EtAl:2018:W18-15,
  author    = {Peng, Nanyun  and  Ghazvininejad, Marjan  and  May, Jonathan  and  Knight, Kevin},
  title     = {Towards Controllable Story Generation},
  booktitle = {Proceedings of the First Workshop on Storytelling},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {43--49},
  abstract  = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
  url       = {http://www.aclweb.org/anthology/W18-1505}
}


@Book{S18-1:2018,
  editor    = {Marianna Apidianaki  and  Saif M. Mohammad  and  Jonathan May  and  Ekaterina Shutova  and  Steven Bethard  and  Marine Carpuat},
  title     = {Proceedings of The 12th International Workshop on Semantic Evaluation},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {http://www.aclweb.org/anthology/S18-1}
}


@InProceedings{zhang-EtAl:2018:N18-5,
  author    = {Zhang, Boliang  and  Lin, Ying  and  Pan, Xiaoman  and  Lu, Di  and  May, Jonathan  and  Knight, Kevin  and  Ji, Heng},
  title     = {ELISA-EDL: A Cross-lingual Entity Extraction, Linking and Localization System},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {41--45},
  abstract  = {We demonstrate ELISA-EDL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets, resources and system training and testing APIs publicly available for research purpose.},
  url       = {http://www.aclweb.org/anthology/N18-5009}
}

@InProceedings{chen-EtAl:2018:N18-14,
  author    = {Chen, Yining  and  Gilroy, Sorcha  and  Maletti, Andreas  and  May, Jonathan  and  Knight, Kevin},
  title     = {Recurrent Neural Networks as Weighted Language Recognizers},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {2261--2271},
  abstract  = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
  url       = {http://www.aclweb.org/anthology/N18-1205}
}

@Article{Hermjakob2018,
author="Hermjakob, Ulf
and Li, Qiang
and Marcu, Daniel
and May, Jonathan
and Mielke, Sebastian J.
and Pourdamghani, Nima
and Pust, Michael
and Shi, Xing
and Knight, Kevin
and Levinboim, Tomer
and Murray, Kenton
and Chiang, David
and Zhang, Boliang
and Pan, Xiaoman
and Lu, Di
and Lin, Ying
and Ji, Heng",
title="Incident-Driven Machine Translation and Name Tagging for Low-resource Languages",
journal="Machine Translation",
year="2018",
month="Jun",
day="01",
volume="32",
number="1",
pages="59--89",
abstract="We describe novel approaches to tackling the problem of natural language processing for low-resource languages. The approaches are embodied in systems for name tagging and machine translation (MT) that we constructed to participate in the NIST LoReHLT evaluation in 2016. Our methods include universal tools, rapid resource and knowledge acquisition, rapid language projection, and joint methods for MT and name tagging.",
issn="1573-0573",
doi="10.1007/s10590-017-9207-1",
url="https://doi.org/10.1007/s10590-017-9207-1"
}

@article{doi:10.1089/big.2017.0012,
author = { Huang Lifu  and  May Jonathan  and  Pan Xiaoman  and  Ji Heng  and  Ren Xiang  and  Han Jiawei  and  Zhao Lin  and  Hendler James A. },
title = {Liberal Entity Extraction: Rapid Construction of Fine-Grained Entity Typing Systems},
journal = {Big Data},
volume = {5},
number = {1},
pages = {19-31},
year = {2017},
doi = {10.1089/big.2017.0012},
    note ={PMID: 28328252},

URL = { 
        https://doi.org/10.1089/big.2017.0012
    
},
eprint = { 
        https://doi.org/10.1089/big.2017.0012
    
}
,
    abstract = { Abstract The ability of automatically recognizing and typing entities in natural language without prior knowledge (e.g., predefined entity types) is a major challenge in processing such data. Most existing entity typing systems are limited to certain domains, genres, and languages. In this article, we propose a novel unsupervised entity-typing framework by combining symbolic and distributional semantics. We start from learning three types of representations for each entity mention: general semantic representation, specific context representation, and knowledge representation based on knowledge bases. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework does not rely on any annotated data, predefined typing schema, or handcrafted features; therefore, it can be quickly adapted to a new domain, genre, and/or language. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework. }
}

@InProceedings{may-priyadarshi:2017:SemEval,
  author    = {May, Jonathan  and  Priyadarshi, Jay},
  title     = {SemEval-2017 Task 9: Abstract Meaning Representation Parsing and Generation},
  booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {536--545},
  abstract  = {In this report we summarize the results of the 2017 AMR SemEval shared task.
	The task consisted of two separate yet related subtasks. In the parsing
	subtask, participants were asked to produce Abstract Meaning Representation
	(AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the
	biomedical domain. In the generation subtask, participants were asked to
	generate English sentences given AMR graphs in the news/forum domain. A total
	of five sites participated in the parsing subtask, and four participated in the
	generation subtask. 
	Along with a description of the task and the participants' systems, we show
	various score ablations and some sample outputs.},
  url       = {http://www.aclweb.org/anthology/S17-2090}
}

@InProceedings{pan-EtAl:2017:Long2,
  author    = {Pan, Xiaoman  and  Zhang, Boliang  and  May, Jonathan  and  Nothman, Joel  and  Knight, Kevin  and  Ji, Heng},
  title     = {Cross-lingual Name Tagging and Linking for 282 Languages},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1946--1958},
  abstract  = {The ambitious goal of this work is to develop a cross-lingual name tagging and
	linking framework for 282 languages that exist in Wikipedia. Given a document
	in any of these languages, our framework is able to identify name mentions,
	assign a coarse-grained or fine-grained type to each mention, and link it to an
	English Knowledge Base (KB) if it is linkable. We achieve this goal by
	performing a series of new KB mining methods: generating ``silver-standard''
	annotations by transferring annotations from English to other languages through
	cross-lingual links and KB properties, refining annotations through
	self-training and topic selection, deriving language-specific morphology
	features from anchor links, and mining word translation pairs from
	cross-lingual links. Both name tagging and linking results for 282 languages
	are promising on Wikipedia data and on-Wikipedia data.},
  url       = {http://aclweb.org/anthology/P17-1178}
}

@inproceedings{Papadopoulos2017,
  author={Pavlos Papadopoulos and Ruchir Travadi and Colin Vaz and Nikolaos Malandrakis and Ulf Hermjakob and Nima Pourdamghani and Michael Pust and Boliang Zhang and Xiaoman Pan and Di Lu and Ying Lin and Ondřej Glembek and Murali Karthick Baskar and Martin Karafiát and Lukáš Burget and Mark Hasegawa-Johnson and Heng Ji and Jonathan May and Kevin Knight and Shrikanth S. Narayanan},
  title={Team ELISA System for DARPA LORELEI Speech Evaluation 2016},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={2053--2057},
  doi={10.21437/Interspeech.2017-180},
  url={http://dx.doi.org/10.21437/Interspeech.2017-180}
}

@InProceedings{zoph-EtAl:2016:EMNLP2016,
  author    = {Zoph, Barret  and  Yuret, Deniz  and  May, Jonathan  and  Knight, Kevin},
  title     = {Transfer Learning for Low-Resource Neural Machine Translation},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {1568--1575},
  url       = {https://aclweb.org/anthology/D16-1163}
}

@InProceedings{may:2016:SemEval,
  author    = {May, Jonathan},
  title     = {SemEval-2016 Task 8: Meaning Representation Parsing},
  booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {1063--1073},
  url       = {http://www.aclweb.org/anthology/S16-1166}
}

@InProceedings{zoph-EtAl:2016:N16-1,
  author    = {Zoph, Barret  and  Vaswani, Ashish  and  May, Jonathan  and  Knight, Kevin},
  title     = {Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {1217--1222},
  url       = {http://www.aclweb.org/anthology/N16-1145}
}

@InProceedings{CHOI16.581,
  author = {Eunsol Choi and Matic Horvat and Jonathan May and Kevin Knight and Daniel Marcu},
  title = {Extracting Structured Scholarly Information from the Machine Translation Literature},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year = {2016},
  month = {may},
  date = {23-28},
  location = {Portorož, Slovenia},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {978-2-9517408-9-1},
  language = {english}
 }

@InProceedings{pust-EtAl:2015:EMNLP,
  author    = {Pust, Michael  and  Hermjakob, Ulf  and  Knight, Kevin  and  Marcu, Daniel  and  May, Jonathan},
  title     = {Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {1143--1154},
  url       = {http://aclweb.org/anthology/D15-1136}
}

@InProceedings{gordon-EtAl:2015:Metaphor2,
  author    = {Gordon, Jonathan  and  Hobbs, Jerry  and  May, Jonathan  and  Mohler, Michael  and  Morbini, Fabrizio  and  Rink, Bryan  and  Tomlinson, Marc  and  Wertheim, Suzanne},
  title     = {A Corpus of Rich Metaphor Annotation},
  booktitle = {Proceedings of the Third Workshop on Metaphor in NLP},
  month     = {June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {56--66},
  url       = {http://www.aclweb.org/anthology/W15-1407}
}

@InProceedings{gordon-EtAl:2015:Metaphor1,
  author    = {Gordon, Jonathan  and  Hobbs, Jerry  and  May, Jonathan  and  Morbini, Fabrizio},
  title     = {High-Precision Abductive Mapping of Multilingual Metaphors},
  booktitle = {Proceedings of the Third Workshop on Metaphor in NLP},
  month     = {June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {50--55},
  url       = {http://www.aclweb.org/anthology/W15-1406}
}

@InProceedings{may-benjira-echihabi:2014:AMTA,
  author    = {May, Jonathan  and  Benjira, Yassine and Echihabi, Abdessamad},
  title     = {An Arabizi-English Social Media Statistical Machine Translation System},
  booktitle = {Proceedings of the Eleventh Biennial Conference of the
Association for Machine Translation in the Americas},
  month     = {October},
  year      = {2014},
  address   = {Vancouver, Canada},
  publisher = {Association for Machine Translation in the Americas},
}

% missing: "Identifying Useful Human Correction Feedback from an On-line Machine Translation Service", (A. Barrón-Cedeño, L. Màrquez, L., C. Henríquez Q., L. Formiga, E. Romero, & J. May), Proc. IJCAI, 2013.

@InProceedings{hopkins-may:2013:ACL2013,
  author    = {Hopkins, Mark  and  May, Jonathan},
  title     = {Models of Translation Competitions},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2013},
  address   = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics},
  pages     = {1416--1424},
  url       = {http://www.aclweb.org/anthology/P13-1139}
}

@InProceedings{PIGHIN12.337,
  author = {Daniele Pighin and Lluís Màrquez and Jonathan May},
  title = {An Analysis (and an Annotated Corpus) of User Responses to Machine Translation Output},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Uğur Doğan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
  language = {english}
}

@InProceedings{hopkins-may:2011:EMNLP,
  author    = {Hopkins, Mark  and  May, Jonathan},
  title     = {Tuning as Ranking},
  booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  month     = {July},
  year      = {2011},
  address   = {Edinburgh, Scotland, UK.},
  publisher = {Association for Computational Linguistics},
  pages     = {1352--1362},
  url       = {http://www.aclweb.org/anthology/D11-1125}
}

@InProceedings{may-knight-vogler:2010:ACL,
  author    = {May, Jonathan  and  Knight, Kevin  and  Vogler, Heiko},
  title     = {Efficient Inference through Cascades of Weighted Tree Transducers},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  month     = {July},
  year      = {2010},
  address   = {Uppsala, Sweden},
  publisher = {Association for Computational Linguistics},
  pages     = {1058--1066},
  url       = {http://www.aclweb.org/anthology/P10-1108}
}

@Article{wangmayknightmarcu10:cl,
  author = 	 {Wei Wang and Jonathan May and Kevin Knight and Daniel Marcu},
  title = 	 {Re-structuring, Re-labeling, and Re-aligning for Syntax-Based Machine Translation },
  journal = 	 {Computational Linguistics},
  year = 	 2010,
  volume =	 36,
  number =	 2,
  pages =	 {247--277},
  month =	 {June}
}

% missing: "Determinization of Weighted Tree Automata using Factorizations", (M. Büchse, J. May, and H. Vogler), Proc. FSMNLP, 2009.
% missing: "Backward and Forward Bisimulation Minimization of Tree Automata", (J. Högberg, A. Maletti, and J. May), Theoretical Computer Science, volume 410, no. 37 (September, 2009).
% missing: "Applications of Weighted Automata in Natural Language Processing", (K. Knight and J. May), Handbook of Weighted Automata (M. Droste, W. Kuich, H. Vogler, eds.), 2009. 

@Article{graehlknightmay08:cl,
  author = 	 {Jonathan Graehl and Kevin Knight and Jonathan May},
  title = 	 {Training Tree Transducers},
  journal = 	 {Computational Linguistics},
  year = 	 2008,
  volume =	 34,
  number =	 3,
  pages =	 {391--427},
  month =	 {September}
}

@InProceedings{mayknight07:emnlp,
  author = 	 {Jonathan May and Kevin Knight},
  title = 	 {Syntactic Re-Alignment Models for Machine Translation},
  booktitle =	 {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  pages =	 {360--368},
  year =	 2007,
  editor =	 {Jason Eisner and Taku Kudo},
  address =	 {Prague, Czech Republic},
  month =	 {June 28 -- June 30},
  publisher =	 {Association for Computational Linguistics}
}

@InProceedings{hogbergmalettimay07:ciaa,
  author = 	 {Johanna H\"{o}gberg and Andreas Maletti and Jonathan May},
  title = 	 {Backward and Forward Bisimulation Minimisation of Weighted Tree Automata},
  booktitle =	 {Proceedings of the 12th International Conference on Implementation and Application of Automata, CIAA 2007},
  pages =	 {109--121},
  year =	 2007,
  editor =	 {Jan Holub and Jan \v{Z}d\'{a}rek},
  volume =	 4783,
  series =	 {Lecture Notes in Computer Science},
  address =	 {Prague, Czech Republic},
  month =	 {July 16--18},
  publisher =	 {Springer-Verlag}
}

@InProceedings{hogbergmalettimay07:dlt,
  author = 	 {Johanna H\"{o}gberg and Andreas Maletti and Jonathan May},
  title = 	 {Bisimulation Minimisation for Weighted Tree Automata},
  booktitle =	 {Proceedings of the 11th International Conference on Developments in Language Theory, DLT 2007},
  pages =	 {229--240},
  year =	 2007,
  editor =	 {Tero Harju and Juhani Karhum\"{a}ki and Arto Lepist\"{o}},
  volume =	 4588,
  series =	 {Lecture Notes in Computer Science},
  address =	 {Turku, Finland},
  month =	 {July 3--6},
  publisher =	 {Springer-Verlag}
}

@InProceedings{mayknight06:ciaa,
  author = 	 {Jonathan May and Kevin Knight},
  title = 	 {Tiburon: A Weighted Tree Automata Toolkit},
  booktitle =	 {Proceedings of the 11th International Conference of Implementation and Application of Automata, CIAA 2006},
  pages =	 {102--113},
  year =	 2006,
  editor =	 {Oscar H. Ibarra and Hsu-Chun Yen},
  volume =	 4094,
  series =	 {Lecture Notes in Computer Science},
  address =	 {Taipei, Taiwan},
  month =	 {August},
  publisher =	 {Springer}
}

@InProceedings{mayknight06:naacl,
  author = 	 {Jonathan May and Kevin Knight},
  title = 	 {A Better N-Best List: Practical Determinization of Weighted Finite Tree Automata},
  booktitle =	 {Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics},
  pages =	 {351--358},
  year =	 2006,
  editor =	 {Sanjeev Khudanpur and Brian Roark},
  volume =	 {Main Proceedings},
  address =	 {Brooklyn, New York},
  month =	 {June 5 -- June 7},
  publisher =	 {Association for Computational Linguistics}
}

@article{979873,
 author = {Jonathan May and Ada Brunstein and Prem Natarajan and Ralph Weischedel},
 title = {Surprise! What's in a Cebuano or Hindi Name?},
 journal = {ACM Transactions on Asian Language Information Processing (TALIP)},
 volume = {2},
 number = {3},
 year = {2003},
 issn = {1530-0226},
 pages = {169--180},
 doi = {http://doi.acm.org/10.1145/979872.979873},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }

@inproceedings{DBLP:conf/ndqa/XuLMMW03,
  author    = {Jinxi Xu and
               Ana Licuanan and
               Jonathan May and
               Scott Miller and
               Ralph M. Weischedel},
  title     = {Answer Selection and Confidence Estimation.},
  booktitle = {New Directions in Question Answering},
  year      = {2003},
  pages     = {134-137},
  crossref  = {DBLP:conf/ndqa/2003},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/ndqa/2003,
  editor    = {Mark T. Maybury},
  title     = {New Directions in Question Answering, Papers from 2003 AAAI
               Spring Symposium, Stanford University, Stanford, CA, USA},
  booktitle = {New Directions in Question Answering},
  publisher = {AAAI Press},
  year      = {2003},
  isbn      = {1-57735-184-3},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{DBLP:conf/trec/XuLMMW02,
  author    = {Jinxi Xu and
               Ana Licuanan and
               Jonathan May and
               Scott Miller and
               Ralph M. Weischedel},
  title     = {TREC 2002 QA at BBN: Answer Selection and Confidence Estimation.},
  booktitle = {TREC},
  year      = {2002},
  ee        = {http://trec.nist.gov/pubs/trec11/papers/bbn.xu.qa.pdf},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{DBLP:journals/corr/PustHKMM15,
  author    = {Michael Pust and
               Ulf Hermjakob and
               Kevin Knight and
               Daniel Marcu and
               Jonathan May},
  title     = {Using Syntax-Based Machine Translation to Parse English into Abstract
               Meaning Representation},
  journal   = {CoRR},
  volume    = {abs/1504.06665},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.06665},
  archivePrefix = {arXiv},
  eprint    = {1504.06665},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PustHKMM15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

